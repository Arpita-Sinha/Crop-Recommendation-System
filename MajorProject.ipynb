{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "2sbWXOrlyuzO",
        "outputId": "bf94da3a-4e75-4859-93ce-cbad66431cf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b16d52e6-b5fb-46c4-a938-7fcbbcb4c719\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b16d52e6-b5fb-46c4-a938-7fcbbcb4c719\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3394593470>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "  from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs7akk8myrLG"
      },
      "outputs": [],
      "source": [
        "# Load the dataset and show its head and columns\n",
        "import pandas as pd\n",
        "\n",
        "# Change the file path to a relative path, assuming the file is in the same directory\n",
        "df = pd.read_csv(\"Crop Recommendation using Soil Properties and Weather Prediction (3).csv\")\n",
        "print(df.head())\n",
        "print('\\nColumns:', list(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mPUMRYT0l6H"
      },
      "outputs": [],
      "source": [
        "# Step 1: Check for missing values\n",
        "import pandas as pd\n",
        "\n",
        "missing = df.isnull().sum()\n",
        "print(missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZnfzU70pf_"
      },
      "outputs": [],
      "source": [
        "# Step 1 Enhancement: generate EC and temp range features, then normalize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Derive EC feature: sum of major nutrients\n",
        "soil_df = df.copy()\n",
        "soil_df['EC'] = soil_df['K'] + soil_df['P'] + soil_df['N']\n",
        "# Seasonal temp range\n",
        "seasons = ['W', 'Sp', 'Su', 'Au']\n",
        "for s in seasons:\n",
        "    soil_df['TRANGE-' + s] = soil_df['T2M_MAX-' + s] - soil_df['T2M_MIN-' + s]\n",
        "\n",
        "# Encode Soilcolor as numeric via one-hot\n",
        "soil_df = pd.get_dummies(soil_df, columns=['Soilcolor'], drop_first=True)\n",
        "\n",
        "# Identify numeric columns to scale (excluding label and dummy soilcolor prefix 'Soilcolor_')\n",
        "numeric_cols = soil_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "numeric_cols.remove('label') if 'label' in numeric_cols else None\n",
        "# Exclude any dummy encoded\n",
        "numeric_cols = [c for c in numeric_cols if not c.startswith('Soilcolor_')]\n",
        "\n",
        "# Scale\n",
        "scaler = MinMaxScaler()\n",
        "soil_df[numeric_cols] = scaler.fit_transform(soil_df[numeric_cols])\n",
        "\n",
        "# Preview normalized data\n",
        "display_df = soil_df.head()\n",
        "print(display_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0NSqCNn0uns"
      },
      "outputs": [],
      "source": [
        "# Step 2: Exploratory Data Analysis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Correlation heatmap of numeric features plus EC and TRANGE features\n",
        "df_corr = soil_df[numeric_cols + ['EC'] + [col for col in soil_df.columns if col.startswith('TRANGE-')]].corr()\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df_corr, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Boxplots for Zn, S, EC, PS\n",
        "plt.figure(figsize=(12,8))\n",
        "for i, col in enumerate(['Zn', 'S', 'EC', 'PS'], 1):\n",
        "    plt.subplot(2,2,i)\n",
        "    sns.boxplot(x=soil_df[col])\n",
        "    plt.title('Boxplot of ' + col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jgywh2K04xE"
      },
      "outputs": [],
      "source": [
        "# Step 3: RFE with Random Forest to select top features and rebuild model\n",
        "# Imports and train-test split\n",
        "tqdm_installed = True\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm_installed = False\n",
        "    %pip install tqdm\n",
        "    from tqdm import tqdm\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Prepare X and y\n",
        "df_model = soil_df.copy()\n",
        "X = df_model.drop(columns=['label'])\n",
        "y = df_model['label']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize RandomForest and RFE\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# Select top 10 features\n",
        "selector = RFE(estimator=rf, n_features_to_select=10, step=1)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "selected_features = X_train.columns[selector.support_].tolist()\n",
        "\n",
        "# Show selected features\n",
        "print('Selected Features:')\n",
        "for feat in selected_features:\n",
        "    print(feat)\n",
        "\n",
        "# Rebuild model using only selected features\n",
        "rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_selected.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = rf_selected.predict(X_test[selected_features])\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Test Accuracy:', acc)\n",
        "\n",
        "# Classification report\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "cm = confusion_matrix(y_test, y_pred, labels=rf_selected.classes_)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=rf_selected.classes_, yticklabels=rf_selected.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy9ARlB11QHu"
      },
      "outputs": [],
      "source": [
        "# Step 4: Hyperparameter tuning for RandomForest on selected features\n",
        "# Using GridSearchCV to optimize n_estimators and max_depth\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Display best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print('Best Parameters:')\n",
        "print(best_params)\n",
        "print('Best CV Accuracy:')\n",
        "print(best_score)\n",
        "\n",
        "# Evaluate tuned model on test set\n",
        "tuned_rf = grid_search.best_estimator_\n",
        "y_pred_tuned = tuned_rf.predict(X_test[selected_features])\n",
        "acc_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print('Test Accuracy after Tuning:')\n",
        "print(acc_tuned)\n",
        "\n",
        "# Show a snippet of cv results\n",
        "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
        "print(cv_results[['param_n_estimators','param_max_depth','param_min_samples_split','mean_test_score']].head())\n",
        "\n",
        "# Confusion matrix for tuned model\n",
        "cm_tuned = confusion_matrix(y_test, y_pred_tuned, labels=tuned_rf.classes_)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_tuned, annot=True, fmt='d', xticklabels=tuned_rf.classes_, yticklabels=tuned_rf.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Tuned RF')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ikcKGkSYT4"
      },
      "outputs": [],
      "source": [
        "# Step 5: Train and evaluate XGBoost on selected features\n",
        "# Install xgboost if needed\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except ImportError:\n",
        "    %pip install xgboost\n",
        "    import xgboost as xgb\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode the target variable y into numerical labels\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Initialize XGBClassifier\n",
        "# Removed use_label_encoder=False as it's deprecated and handled by the LabelEncoder\n",
        "xgb_clf = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Cross-validation on training set using the encoded labels\n",
        "grid_scores = cross_val_score(xgb_clf, X_train[selected_features], y_train_encoded, cv=5, scoring='accuracy')\n",
        "print('XGBoost CV Accuracy Scores:')\n",
        "print(grid_scores)\n",
        "print('Mean CV Accuracy:')\n",
        "print(grid_scores.mean())\n",
        "\n",
        "# Fit on full training data using the encoded labels\n",
        "xgb_clf.fit(X_train[selected_features], y_train_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb_encoded = xgb_clf.predict(X_test[selected_features])\n",
        "\n",
        "# Decode the predictions back to original labels for evaluation and reporting\n",
        "y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
        "\n",
        "# Evaluate on test set using the original y_test and decoded predictions\n",
        "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print('XGBoost Test Accuracy:')\n",
        "print(acc_xgb)\n",
        "\n",
        "# Classification report using original labels\n",
        "print('Classification Report - XGBoost:')\n",
        "# Ensure that the labels in y_test and y_pred_xgb are consistent for classification_report\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Confusion matrix heatmap using original labels\n",
        "# Use the classes from the LabelEncoder as labels for the confusion matrix\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb, labels=le.classes_)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - XGBoost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgOUXSPzZhCa"
      },
      "outputs": [],
      "source": [
        "# Install xgboost and then run the evaluation with XGBoost\n",
        "# Installing xgboost\n",
        "%pip install xgboost\n",
        "\n",
        "# Imports and training\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder # Ensure LabelEncoder is imported if not already\n",
        "\n",
        "# Initialize LabelEncoder if not already in the current cell\n",
        "# This assumes le is available from the previous cell\n",
        "# If running this cell independently, you would need to re-initialize and fit the LabelEncoder\n",
        "# le = LabelEncoder()\n",
        "# y_train_encoded = le.fit_transform(y_train)\n",
        "# y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Re-encode y_train using the existing LabelEncoder instance 'le'\n",
        "# This ensures y_train is numerical as required by XGBoost\n",
        "y_train_encoded = le.transform(y_train)\n",
        "\n",
        "\n",
        "# Removed use_label_encoder=False as it's deprecated and handled by LabelEncoder\n",
        "xgb_clf = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Cross-validation\n",
        "# Use the encoded y_train for cross-validation\n",
        "cv_scores_xgb = cross_val_score(xgb_clf, X_train[selected_features], y_train_encoded, cv=5, scoring='accuracy')\n",
        "print('CV Scores XGBoost:')\n",
        "print(cv_scores_xgb)\n",
        "print('Mean CV Accuracy:')\n",
        "print(cv_scores_xgb.mean())\n",
        "\n",
        "# Fit and evaluate\n",
        "# Use the encoded y_train for fitting\n",
        "xgb_clf.fit(X_train[selected_features], y_train_encoded)\n",
        "\n",
        "# Predict on the test set (features only, prediction will be encoded)\n",
        "y_pred_xgb_encoded = xgb_clf.predict(X_test[selected_features])\n",
        "\n",
        "# Decode the predictions back to original labels\n",
        "y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
        "\n",
        "\n",
        "# Evaluate on test set using the original y_test and decoded predictions\n",
        "# Note: While y_test itself is not used for prediction input, it's needed here for comparison\n",
        "# to the decoded predictions y_pred_xgb\n",
        "acc_xgb_test = accuracy_score(y_test, y_pred_xgb)\n",
        "print('Test Accuracy XGBoost:')\n",
        "print(acc_xgb_test)\n",
        "\n",
        "# Report and confusion\n",
        "print('Classification Report - XGBoost:')\n",
        "# Use the original y_test and the decoded y_pred_xgb for the classification report\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Confusion matrix using original labels (le.classes_)\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb, labels=le.classes_)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - XGBoost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTRcp44xVuXv"
      },
      "outputs": [],
      "source": [
        "# Installing and running XGBoost evaluation on selected features\n",
        "# Install xgboost if not already installed\n",
        "import importlib\n",
        "\n",
        "if not importlib.util.find_spec('xgboost'):\n",
        "    %pip install xgboost\n",
        "\n",
        "# Imports\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder # Ensure LabelEncoder is imported\n",
        "\n",
        "# Initialize LabelEncoder and encode the target variable\n",
        "# We need to fit the encoder again in this cell if it's run independently\n",
        "# If the previous cells (9 or 10) were run successfully, the `le` object might exist\n",
        "# and `y_train` and `y_test` are already loaded, but not encoded in THIS cell scope.\n",
        "# The safest way for a standalone cell is to re-encode.\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test) # Also encode test set for potential future use, though not strictly needed for THIS fit/cross_val_score\n",
        "\n",
        "# Initialize classifier\n",
        "# use_label_encoder=False is acceptable, but XGBoost still requires numerical labels.\n",
        "# eval_metric='mlogloss' is good for multi-class classification.\n",
        "xgb_clf = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Cross-validation on training set using the encoded labels\n",
        "# Use y_train_encoded here\n",
        "cv_scores_xgb = cross_val_score(xgb_clf, X_train[selected_features], y_train_encoded, cv=5, scoring='accuracy')\n",
        "print('XGBoost CV Scores:')\n",
        "print(cv_scores_xgb)\n",
        "print('Mean CV Accuracy:')\n",
        "print(cv_scores_xgb.mean())\n",
        "\n",
        "# Fit on training data using the encoded labels\n",
        "# Use y_train_encoded here\n",
        "xgb_clf.fit(X_train[selected_features], y_train_encoded)\n",
        "\n",
        "# Predict on test set (prediction will be encoded)\n",
        "y_pred_xgb_encoded = xgb_clf.predict(X_test[selected_features])\n",
        "\n",
        "# Decode the predictions back to original labels for evaluation\n",
        "y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
        "\n",
        "# Evaluate on test set using the original y_test and decoded predictions\n",
        "# Note: Although y_test is not used for prediction input, it is needed here\n",
        "# for comparison with the decoded predictions y_pred_xgb\n",
        "acc_xgb_test = accuracy_score(y_test, y_pred_xgb)\n",
        "print('XGBoost Test Accuracy:')\n",
        "print(acc_xgb_test)\n",
        "\n",
        "# Detailed classification report using original labels\n",
        "print('Classification Report - XGBoost:')\n",
        "# Use the original y_test and the decoded y_pred_xgb for the classification report\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Confusion matrix heatmap using original labels (le.classes_)\n",
        "# Use the classes from the fitted LabelEncoder for consistent axis labels\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb, labels=le.classes_)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - XGBoost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmVm5vPdbTLe"
      },
      "outputs": [],
      "source": [
        "# Step 6: Build and train a simple Keras neural network on the selected features\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "# It's best to fit on the combined unique labels to ensure all classes are covered\n",
        "# Assuming 'label' column is available in original df or soil_df\n",
        "# If not, fit on y_train and y_test combined or separately, being careful\n",
        "# Here we assume the 'le' object from previous cells is available and fitted\n",
        "# If running this cell independently, you might need to re-fit LabelEncoder\n",
        "try:\n",
        "    # Try to use the LabelEncoder from previous cells if available\n",
        "    le\n",
        "except NameError:\n",
        "    # If le is not defined, initialize and fit it now\n",
        "    # Need access to the original full label list or combine y_train/y_test values\n",
        "    # Assuming 'soil_df' (or a similar df with the 'label' column) is available\n",
        "    # or y_train/y_test together cover all possible labels.\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    # Fit on all labels found in both train and test splits\n",
        "    all_labels = pd.concat([y_train, y_test]).unique()\n",
        "    le.fit(all_labels)\n",
        "\n",
        "\n",
        "# Encode the target variables\n",
        "y_train_encoded = le.transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Define model architecture\n",
        "model = models.Sequential()\n",
        "model.add(layers.Input(shape=(len(selected_features),)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "# Use the number of unique encoded classes for the output layer\n",
        "model.add(layers.Dense(len(le.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "# Use sparse_categorical_crossentropy with integer labels\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model using the *encoded* target variable\n",
        "history = model.fit(\n",
        "    X_train[selected_features], y_train_encoded, # Use y_train_encoded here\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate baseline performance on the test set using the *encoded* target variable\n",
        "test_loss, test_acc = model.evaluate(X_test[selected_features], y_test_encoded, verbose=0) # Use y_test_encoded here\n",
        "print('Baseline NN Test Loss:')\n",
        "print(test_loss)\n",
        "print('Baseline NN Test Accuracy:')\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyCwxuaG3uqw"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow if not present\n",
        "import importlib\n",
        "if not importlib.util.find_spec('tensorflow'):\n",
        "    %pip install tensorflow\n",
        "print('TensorFlow installation check complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD4HOwi_bfll"
      },
      "outputs": [],
      "source": [
        "# Build, train, and evaluate a simple Keras neural network\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder # Ensure LabelEncoder is imported if not already used\n",
        "\n",
        "# --- Start of Added/Modified Code ---\n",
        "\n",
        "# Re-initialize and fit LabelEncoder if needed, or ensure it's available\n",
        "# Assuming 'le' from previous cells is available. If running this cell\n",
        "# independently, you would need to fit it on the complete set of labels.\n",
        "# A robust approach is to fit it on the original full label column.\n",
        "try:\n",
        "    le\n",
        "except NameError:\n",
        "    # If le is not defined, re-fit it.\n",
        "    # This assumes you have access to the original data or combined y_train/y_test\n",
        "    print(\"LabelEncoder 'le' not found, initializing and fitting.\")\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    # Assuming access to the original DataFrame 'df' or soil_df\n",
        "    # Or fit on the combined unique labels from y_train and y_test\n",
        "    # Here we'll fit on the combined y_train and y_test for robustness in this cell\n",
        "    import pandas as pd\n",
        "    all_labels = pd.concat([y_train, y_test]).unique()\n",
        "    le.fit(all_labels)\n",
        "\n",
        "\n",
        "# Encode the target variables to integers\n",
        "y_train_encoded = le.transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# --- End of Added/Modified Code ---\n",
        "\n",
        "# Model definition\n",
        "nn_model = models.Sequential([\n",
        "    layers.Input(shape=(len(selected_features),)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    # Use the number of unique classes from the LabelEncoder for the output layer\n",
        "    layers.Dense(len(le.classes_), activation='softmax') # Use len(le.classes_)\n",
        "])\n",
        "nn_model.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "# Early stopping\n",
        "es = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "# Initial training\n",
        "nn_history = nn_model.fit(\n",
        "    X_train[selected_features],\n",
        "    y_train_encoded,  # Use the encoded target variable here\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "# Plot history\n",
        "dict_hist = nn_history.history\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(dict_hist['loss'], label='train_loss')\n",
        "plt.plot(dict_hist['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over Epochs')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(dict_hist['accuracy'], label='train_acc')\n",
        "plt.plot(dict_hist['val_accuracy'], label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Baseline evaluation\n",
        "# Evaluate using the encoded test target variable\n",
        "baseline_loss, baseline_acc = nn_model.evaluate(X_test[selected_features], y_test_encoded, verbose=0) # Use y_test_encoded here\n",
        "print('Baseline NN Test Loss:')\n",
        "print(baseline_loss)\n",
        "print('Baseline NN Test Accuracy:')\n",
        "print(baseline_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q6csM3kbujs"
      },
      "outputs": [],
      "source": [
        " # Compare RF (tuned) vs NN baseline: compute metrics and plot confusion matrices\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder # Ensure LabelEncoder is imported\n",
        "\n",
        "# Predict with NN on test set\n",
        "# The raw output is probabilities, argmax gets the integer class index\n",
        "y_pred_nn_encoded = nn_model.predict(X_test[selected_features]).argmax(axis=1)\n",
        "\n",
        "# --- Start of Added/Modified Code ---\n",
        "\n",
        "# Decode the integer predictions back to original string labels\n",
        "# Assuming 'le' LabelEncoder from previous cells is available and fitted on all classes\n",
        "try:\n",
        "    le\n",
        "except NameError:\n",
        "    # If le is not defined, re-fit it here for robustness\n",
        "    print(\"LabelEncoder 'le' not found. Re-initializing and fitting on all unique labels.\")\n",
        "    le = LabelEncoder()\n",
        "    import pandas as pd # Ensure pandas is imported\n",
        "    # Fit on the combined unique labels from y_train and y_test\n",
        "    all_labels = pd.concat([y_train, y_test]).unique()\n",
        "    le.fit(all_labels)\n",
        "\n",
        "y_pred_nn = le.inverse_transform(y_pred_nn_encoded)\n",
        "\n",
        "# --- End of Added/Modified Code ---\n",
        "\n",
        "\n",
        "def get_metrics(y_true, y_pred, average='weighted'):\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred, average=average, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, average=average, zero_division=0),\n",
        "        'F1 Score': f1_score(y_true, y_pred, average=average, zero_division=0)\n",
        "    }\n",
        "\n",
        "# RF tuned predictions exist as y_pred_rf and model tuned_rf\n",
        "# If not, generate them:\n",
        "try:\n",
        "    y_pred_rf\n",
        "except NameError:\n",
        "    # This assumes tuned_rf and X_test[selected_features] are available\n",
        "    print(\"y_pred_rf not found, generating predictions using tuned_rf.\")\n",
        "    y_pred_rf = tuned_rf.predict(X_test[selected_features])\n",
        "except NameError:\n",
        "     # If tuned_rf is also not found, train a default RF for comparison (less ideal)\n",
        "     print(\"tuned_rf not found. Training a default RandomForestClassifier for comparison.\")\n",
        "     from sklearn.ensemble import RandomForestClassifier\n",
        "     rf_default = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "     # Need original X_train/y_train if tuned_rf was not found\n",
        "     try:\n",
        "         X_train, y_train\n",
        "     except NameError:\n",
        "         print(\"X_train or y_train not found. Cannot train default RF. Please ensure previous cells ran.\")\n",
        "         # Optionally, exit or raise an error if critical variables are missing\n",
        "         raise\n",
        "\n",
        "     rf_default.fit(X_train[selected_features], y_train)\n",
        "     y_pred_rf = rf_default.predict(X_test[selected_features])\n",
        "\n",
        "\n",
        "# Metrics\n",
        "metrics_rf = get_metrics(y_test, y_pred_rf)\n",
        "metrics_nn = get_metrics(y_test, y_pred_nn) # Now y_pred_nn are string labels\n",
        "\n",
        "# Comparison table\n",
        "df_comp = pd.DataFrame([\n",
        "    {'Model': 'Random Forest (tuned)', **metrics_rf},\n",
        "    {'Model': 'Neural Network (baseline)', **metrics_nn}\n",
        "])\n",
        "\n",
        "# Print table\n",
        "print(df_comp)\n",
        "\n",
        "# Plot confusion matrices\n",
        "def plot_cm(y_true, y_pred, title):\n",
        "    # Calculate confusion matrix - sklearn can handle string labels\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Get unique sorted labels for heatmap axes\n",
        "    labels = sorted(pd.concat([pd.Series(y_true), pd.Series(y_pred)]).unique())\n",
        "\n",
        "    plt.figure(figsize=(len(labels)*0.8, len(labels)*0.7)) # Adjust size based on number of classes\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "# Pass the original string labels and the decoded NN predictions to the plot function\n",
        "plot_cm(y_test, y_pred_rf, 'Confusion Matrix - Random Forest (tuned)')\n",
        "plot_cm(y_test, y_pred_nn, 'Confusion Matrix - Neural Network (baseline)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og0OMkO833Ay"
      },
      "outputs": [],
      "source": [
        "# Install keras-tuner if not installed\n",
        "import importlib\n",
        "if not importlib.util.find_spec('kerastuner'):\n",
        "    %pip install keras-tuner\n",
        "print('Keras Tuner installation check complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh93G_bab797"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Keras model using Keras Tuner\n",
        "import kerastuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from sklearn.preprocessing import LabelEncoder # Ensure LabelEncoder is imported\n",
        "\n",
        "# --- Start of Added/Modified Code ---\n",
        "\n",
        "# Re-initialize and fit LabelEncoder if needed, or ensure it's available\n",
        "# This section is for robustness if the cell is run independently.\n",
        "# If previous cells defining and fitting 'le' were run, this block will\n",
        "# check if 'le' exists. If not, it re-initializes and fits it.\n",
        "try:\n",
        "    le\n",
        "except NameError:\n",
        "    print(\"LabelEncoder 'le' not found. Re-initializing and fitting on all unique labels.\")\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import pandas as pd # Ensure pandas is imported\n",
        "    le = LabelEncoder()\n",
        "    # Fit on the combined unique labels from y_train and y_test\n",
        "    # This assumes y_train and y_test (original string labels) are available\n",
        "    try:\n",
        "        all_labels = pd.concat([y_train, y_test]).unique()\n",
        "        le.fit(all_labels)\n",
        "    except NameError:\n",
        "        print(\"Could not re-fit LabelEncoder: y_train or y_test not found.\")\n",
        "        print(\"Please ensure preceding cells defining y_train and y_test are run.\")\n",
        "        # You might want to raise an error or exit here if essential variables are missing\n",
        "        raise\n",
        "\n",
        "\n",
        "# Encode the target variable to integers for Keras Tuner\n",
        "# Use the existing 'le' object to transform y_train\n",
        "y_train_encoded = le.transform(y_train)\n",
        "\n",
        "# Define the number of output classes for the Keras model\n",
        "# This should be based on the number of unique labels in the encoded target\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# --- End of Added/Modified Code ---\n",
        "\n",
        "\n",
        "# Define the model builder function for tuner\n",
        "# Modify the output layer to use 'num_classes'\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    # Input layer\n",
        "    # Ensure selected_features is available; it should be from RFE step\n",
        "    try:\n",
        "        input_shape = (len(selected_features),)\n",
        "    except NameError:\n",
        "        print(\"Error: 'selected_features' is not defined. Please run the RFE step.\")\n",
        "        # Handle this error, perhaps by raising it or returning None\n",
        "        raise\n",
        "\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "    # Tune number of hidden layers: 1 to 3\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        units = hp.Choice(f'units_{i}', [32, 64, 128])\n",
        "        activation = hp.Choice(f'activation_{i}', ['relu', 'tanh'])\n",
        "        model.add(layers.Dense(units, activation=activation))\n",
        "    # Output layer\n",
        "    # Use num_classes (derived from encoded labels) for the output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    # Tune learning rate for optimizer\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner with RandomSearch\n",
        "# You might want to adjust max_trials based on computational resources\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10, # Keep trials relatively low for demonstration\n",
        "    executions_per_trial=1,\n",
        "    directory='tuner_dir',\n",
        "    project_name='crop_nn_tuning'\n",
        ")\n",
        "\n",
        "# EarlyStopping callback\n",
        "# Monitor validation accuracy and stop if it doesn't improve\n",
        "stop_early = callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True) # Added restore_best_weights\n",
        "\n",
        "# Run the hyperparameter search using the *encoded* target variable\n",
        "tuner.search(\n",
        "    X_train[selected_features], # Use the selected features for X\n",
        "    y_train_encoded,            # Use the encoded target variable here\n",
        "    epochs=50,                  # Max epochs per trial\n",
        "    validation_split=0.2,       # Use a split of the training data for validation during tuning\n",
        "    callbacks=[stop_early],     # Apply early stopping\n",
        "    batch_size=32,              # Batch size for training\n",
        "    verbose=1                   # Show progress during search\n",
        ")\n",
        "\n",
        "# Summarize results\n",
        "print(\"\\nSearch results summary:\")\n",
        "tuner.results_summary()\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "# Save best hyperparameters to a JSON file\n",
        "import json\n",
        "best_hps_dict = best_hps.values\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(best_hps_dict)\n",
        "with open('best_hps.json', 'w') as f:\n",
        "    json.dump(best_hps_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yR5rrYz3_Od"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install tensorflow to support Keras Tuner\n",
        "# Checking and installing TensorFlow if missing\n",
        "import importlib\n",
        "if not importlib.util.find_spec('tensorflow'):\n",
        "    %pip install tensorflow\n",
        "print('TensorFlow installation check complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5vsAPSl4BtP"
      },
      "outputs": [],
      "source": [
        "# Load data and display head and summary\n",
        "import pandas as pd\n",
        "# Load dataset\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "# Display head and basic description\n",
        "print(df.head())\n",
        "print(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PNBQNiK4ESX"
      },
      "outputs": [],
      "source": [
        "# Cell: Load dataset and display head and dtypes for initial inspection\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into df\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "\n",
        "# Display the first five rows and the data types of each column\n",
        "df_head = df.head()\n",
        "df_dtypes = df.dtypes\n",
        "\n",
        "# Print results separately\n",
        "def display_results():\n",
        "    print(df_head)\n",
        "    print('\\nColumn Data Types:')\n",
        "    print(df_dtypes)\n",
        "\n",
        "# Invoke display\n",
        "display_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2h38_Hy4G5y"
      },
      "outputs": [],
      "source": [
        "# Step 7: Compare Random Forest vs Neural Network\n",
        "# 1. Preprocessing, splitting, training RF and NN with best hyperparameters\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load data (df is already in memory if re-running; otherwise reload)\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "\n",
        "# Encode categorical columns\n",
        "soil_le = LabelEncoder()\n",
        "df['Soilcolor_enc'] = soil_le.fit_transform(df['Soilcolor'])\n",
        "\n",
        "# Encode label\n",
        "target_le = LabelEncoder()\n",
        "df['label_enc'] = target_le.fit_transform(df['label'])\n",
        "\n",
        "# Define features and target\n",
        "def_cols = [col for col in df.columns if col not in ['Soilcolor','label','label_enc']]\n",
        "X = df[def_cols]\n",
        "y = df['label_enc']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# RF model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# RF predictions and metrics\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "rf_prec = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "rf_rec = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "# Scale features for NN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build NN using best hyperparameters: 2 layers [64, relu], [32, tanh], lr=0.001\n",
        "def build_best_model(input_dim, num_classes):\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(input_dim,)))\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dense(32, activation='tanh'))\n",
        "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "num_classes = len(target_le.classes_)\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "nn = build_best_model(input_dim, num_classes)\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = nn.fit(X_train_scaled, y_train,\n",
        "                 validation_split=0.2,\n",
        "                 epochs=50,\n",
        "                 batch_size=32,\n",
        "                 callbacks=[early_stop],\n",
        "                 verbose=0)\n",
        "\n",
        "# NN predictions and metrics\n",
        "y_pred_nn_prob = nn.predict(X_test_scaled)\n",
        "y_pred_nn = np.argmax(y_pred_nn_prob, axis=1)\n",
        "nn_acc = accuracy_score(y_test, y_pred_nn)\n",
        "nn_prec = precision_score(y_test, y_pred_nn, average='weighted')\n",
        "nn_rec = recall_score(y_test, y_pred_nn, average='weighted')\n",
        "nn_f1 = f1_score(y_test, y_pred_nn, average='weighted')\n",
        "\n",
        "# Create comparison table\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'Neural Network'],\n",
        "    'Accuracy': [rf_acc, nn_acc],\n",
        "    'Precision': [rf_prec, nn_prec],\n",
        "    'Recall': [rf_rec, nn_rec],\n",
        "    'F1 Score': [rf_f1, nn_f1]\n",
        "})\n",
        "\n",
        "print(metrics_df)\n",
        "\n",
        "# Plot confusion matrices\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
        "axes[0].set_title('RF Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('True')\n",
        "\n",
        "sns.heatmap(cm_nn, annot=True, fmt='d', ax=axes[1], cmap='Greens')\n",
        "axes[1].set_title('NN Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('True')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Comparison and confusion matrices generated.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLayvqf74Kkd"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow for NN\n",
        "# Checking if tensorflow is available\n",
        "import importlib\n",
        "if importlib.util.find_spec('tensorflow') is None:\n",
        "    %pip install tensorflow\n",
        "print('TensorFlow installation step completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obaMK39P4Ohn"
      },
      "outputs": [],
      "source": [
        "# Rerun RF and NN comparison now that TensorFlow is installed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "# Encode\n",
        "df['Soilcolor_enc'] = LabelEncoder().fit_transform(df['Soilcolor'])\n",
        "df['label_enc'] = LabelEncoder().fit_transform(df['label'])\n",
        "# Features/target\n",
        "X = df.drop(columns=['Soilcolor','label','label_enc'])\n",
        "y = df['label_enc']\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# RF\n",
        "tf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "tf_rf.fit(X_train, y_train)\n",
        "y_pred_rf = tf_rf.predict(X_test)\n",
        "# Metrics RF\n",
        "rf_acc=accuracy_score(y_test,y_pred_rf)\n",
        "rf_prec=precision_score(y_test,y_pred_rf,average='weighted')\n",
        "rf_rec=recall_score(y_test,y_pred_rf,average='weighted')\n",
        "rf_f1=f1_score(y_test,y_pred_rf,average='weighted')\n",
        "# Scale for NN\n",
        "scaler=StandardScaler().fit(X_train)\n",
        "X_train_s=scaler.transform(X_train)\n",
        "X_test_s=scaler.transform(X_test)\n",
        "# NN\n",
        "def build(input_dim, num_classes):\n",
        "    model=keras.Sequential([keras.layers.Input(shape=(input_dim,)),\n",
        "                             keras.layers.Dense(64,activation='relu'),\n",
        "                             keras.layers.Dense(32,activation='tanh'),\n",
        "                             keras.layers.Dense(num_classes,activation='softmax')])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "num_classes=len(df['label_enc'].unique())\n",
        "model=build(X_train_s.shape[1],num_classes)\n",
        "e=keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=5,restore_best_weights=True)\n",
        "model.fit(X_train_s,y_train,validation_split=0.2,epochs=50,batch_size=32,callbacks=[e],verbose=0)\n",
        "# Predictions NN\n",
        "y_pred_nn=np.argmax(model.predict(X_test_s),axis=1)\n",
        "nn_acc=accuracy_score(y_test,y_pred_nn)\n",
        "nn_prec=precision_score(y_test,y_pred_nn,average='weighted')\n",
        "nn_rec=recall_score(y_test,y_pred_nn,average='weighted')\n",
        "nn_f1=f1_score(y_test,y_pred_nn,average='weighted')\n",
        "# Table\n",
        "metrics_df=pd.DataFrame({'Model':['Random Forest','Neural Network'],\n",
        "                          'Accuracy':[rf_acc,nn_acc],\n",
        "                          'Precision':[rf_prec,nn_prec],\n",
        "                          'Recall':[rf_rec,nn_rec],\n",
        "                          'F1 Score':[rf_f1,nn_f1]})\n",
        "print(metrics_df)\n",
        "# Confusion matrices\n",
        "cm_rf=confusion_matrix(y_test,y_pred_rf)\n",
        "cm_nn=confusion_matrix(y_test,y_pred_nn)\n",
        "fig,axes=plt.subplots(1,2,figsize=(12,5))\n",
        "sns.heatmap(cm_rf,annot=True,fmt='d',ax=axes[0],cmap='Blues')\n",
        "axes[0].set_title('RF Conf Matrix');axes[0].set_xlabel('Pred');axes[0].set_ylabel('True')\n",
        "sns.heatmap(cm_nn,annot=True,fmt='d',ax=axes[1],cmap='Greens')\n",
        "axes[1].set_title('NN Conf Matrix');axes[1].set_xlabel('Pred');axes[1].set_ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY1a9z1g4S_T"
      },
      "outputs": [],
      "source": [
        "# Compare Random Forest vs MLPClassifier as Neural Network fallback\n",
        "# 1. Load and encode data\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "# Encode categorical\n",
        "df['Soilcolor_enc'] = LabelEncoder().fit_transform(df['Soilcolor'])\n",
        "df['label_enc'] = LabelEncoder().fit_transform(df['label'])\n",
        "# Features and target\n",
        "X = df.drop(columns=['Soilcolor','label','label_enc'])\n",
        "y = df['label_enc']\n",
        "# Split\n",
        "tX, tX_test, ty, ty_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(tX, ty)\n",
        "pred_rf = rf.predict(tX_test)\n",
        "# RF metrics\n",
        "rf_acc = accuracy_score(ty_test, pred_rf)\n",
        "rf_prec = precision_score(ty_test, pred_rf, average='weighted')\n",
        "rf_rec = recall_score(ty_test, pred_rf, average='weighted')\n",
        "rf_f1 = f1_score(ty_test, pred_rf, average='weighted')\n",
        "\n",
        "# Scale for MLP\n",
        "scaler = StandardScaler().fit(tX)\n",
        "tX_s = scaler.transform(tX)\n",
        "tX_test_s = scaler.transform(tX_test)\n",
        "# MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64,32), activation='relu', solver='adam', learning_rate_init=0.001, max_iter=200, random_state=42)\n",
        "mlp.fit(tX_s, ty)\n",
        "pred_mlp = mlp.predict(tX_test_s)\n",
        "# MLP metrics\n",
        "mlp_acc = accuracy_score(ty_test, pred_mlp)\n",
        "mlp_prec = precision_score(ty_test, pred_mlp, average='weighted')\n",
        "mlp_rec = recall_score(ty_test, pred_mlp, average='weighted')\n",
        "mlp_f1 = f1_score(ty_test, pred_mlp, average='weighted')\n",
        "\n",
        "# Comparison table\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'Neural Network (MLP)'],\n",
        "    'Accuracy': [rf_acc, mlp_acc],\n",
        "    'Precision': [rf_prec, mlp_prec],\n",
        "    'Recall': [rf_rec, mlp_rec],\n",
        "    'F1 Score': [rf_f1, mlp_f1]\n",
        "})\n",
        "print(metrics_df)\n",
        "\n",
        "# Confusion matrices\n",
        "cm_rf = confusion_matrix(ty_test, pred_rf)\n",
        "cm_mlp = confusion_matrix(ty_test, pred_mlp)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
        "axes[0].set_title('Random Forest Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('True')\n",
        "sns.heatmap(cm_mlp, annot=True, fmt='d', ax=axes[1], cmap='Greens')\n",
        "axes[1].set_title('MLP Neural Network Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Completed comparison table and confusion matrices.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr_Qsxlx4WGq"
      },
      "outputs": [],
      "source": [
        "# Create a Gradio frontend for crop prediction using the trained Random Forest model\n",
        "\n",
        "# Install gradio if not installed\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ModuleNotFoundError:\n",
        "    !pip install gradio\n",
        "    import gradio as gr # Import after installation\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def load_resources():\n",
        "    # Load dataset to get feature names and encoder\n",
        "    df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "    le_color = LabelEncoder().fit(df['Soilcolor'])\n",
        "    le_label = LabelEncoder().fit(df['label'])\n",
        "    # Prepare data\n",
        "    df['Soilcolor_enc'] = le_color.transform(df['Soilcolor'])\n",
        "    # Encode label column BEFORE dropping it - FIX\n",
        "    df['label_enc'] = le_label.transform(df['label'])\n",
        "    X = df.drop(columns=['Soilcolor','label','label_enc'])\n",
        "    scaler = StandardScaler().fit(X)\n",
        "    # Train RF again (or load a persisted model)\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    # Use the encoded label for training\n",
        "    y = df['label_enc'] # Use the newly created 'label_enc' column\n",
        "    rf.fit(X, y)\n",
        "    return rf, scaler, le_color, le_label, X.columns.tolist()\n",
        "\n",
        "rf_model, scaler, le_color, le_label, feature_names = load_resources()\n",
        "\n",
        "# Prediction function for Gradio\n",
        "def predict_crop(pH, Nitrogen, Phosphorous, Potassium, Rainfall, Temperature, Humidity, Soilcolor):\n",
        "    # Encode soilcolor\n",
        "    sc_enc = int(le_color.transform([Soilcolor])[0])\n",
        "    # Prepare feature vector\n",
        "    features = [pH, Nitrogen, Phosphorous, Potassium, Rainfall, Temperature, Humidity, sc_enc]\n",
        "    # Scale\n",
        "    features_scaled = scaler.transform([features])\n",
        "    # Predict\n",
        "    pred_enc = rf_model.predict(features_scaled)[0]\n",
        "    crop = le_label.inverse_transform([pred_enc])[0]\n",
        "    return crop\n",
        "\n",
        "# Build Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_crop,\n",
        "    inputs=[\n",
        "        gr.Number(label='pH'),\n",
        "        gr.Number(label='Nitrogen'),\n",
        "        gr.Number(label='Phosphorous'),\n",
        "        gr.Number(label='Potassium'),\n",
        "        gr.Number(label='Rainfall'),\n",
        "        gr.Number(label='Temperature'),\n",
        "        gr.Number(label='Humidity'),\n",
        "        gr.Dropdown(choices=le_color.classes_.tolist(), label='Soil Color')\n",
        "    ],\n",
        "    outputs=gr.Textbox(label='Recommended Crop'),\n",
        "    title='Crop Recommendation System',\n",
        "    description='Enter soil and weather parameters to get a recommended crop.'\n",
        ")\n",
        "\n",
        "demo.launch(server_name='0.0.0.0')\n",
        "print('Gradio frontend created and running.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLyQiqMZ4XoH"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Gradio\n",
        "# Installing gradio if not present\n",
        "iimport = None  # placeholder to check import\n",
        "try:\n",
        "    import gradio\n",
        "    print('Gradio already installed')\n",
        "except ModuleNotFoundError:\n",
        "    import sys\n",
        "    from subprocess import run\n",
        "    run([sys.executable, '-m', 'pip', 'install', 'gradio'], check=True)\n",
        "    print('Installed gradio')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6VKlLD-4eU7"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import Gradio and build the interface object for the crop recommender\n",
        "import gradio as gr\n",
        "\n",
        "def build_gradio_interface(rf_model, scaler, le_color, le_label):\n",
        "    def predict_crop(pH, Nitrogen, Phosphorous, Potassium, Rainfall, Temperature, Humidity, Soilcolor):\n",
        "        sc_enc = int(le_color.transform([Soilcolor])[0])\n",
        "        features = [pH, Nitrogen, Phosphorous, Potassium, Rainfall, Temperature, Humidity, sc_enc]\n",
        "        features_scaled = scaler.transform([features])\n",
        "        pred_enc = rf_model.predict(features_scaled)[0]\n",
        "        return le_label.inverse_transform([pred_enc])[0]\n",
        "    interface = gr.Interface(\n",
        "        fn=predict_crop,\n",
        "        inputs=[\n",
        "            gr.Number(label='pH'),\n",
        "            gr.Number(label='Nitrogen'),\n",
        "            gr.Number(label='Phosphorous'),\n",
        "            gr.Number(label='Potassium'),\n",
        "            gr.Number(label='Rainfall'),\n",
        "            gr.Number(label='Temperature'),\n",
        "            gr.Number(label='Humidity'),\n",
        "            gr.Dropdown(choices=le_color.classes_.tolist(), label='Soil Color')\n",
        "        ],\n",
        "        outputs=gr.Textbox(label='Recommended Crop'),\n",
        "        title='Crop Recommendation System',\n",
        "        description='Enter soil and weather parameters to get a recommended crop.'\n",
        "    )\n",
        "    return interface\n",
        "\n",
        "# Build the interface\n",
        "demo = build_gradio_interface(rf_model, scaler, le_color, le_label)\n",
        "print('Gradio interface object created.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w99hJZ44hOi"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Load data, preprocess, and train Random Forest model\n",
        "# We load the CSV, encode categorical features, standardize, and fit a RandomForestClassifier.\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "# Initialize encoders\n",
        "e_color = LabelEncoder().fit(df['Soilcolor'])\n",
        "e_label = LabelEncoder().fit(df['label'])\n",
        "# Encode and prepare features\n",
        "df['Soilcolor_enc'] = e_color.transform(df['Soilcolor'])\n",
        "X = df[['Ph','N','P','K','rainfall','temperature','humidity','Soilcolor_enc']] # Changed 'pH' to 'Ph' y = e_label.transform(df['label'])\n",
        "y = e_label.transform(df['label'])\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_scaled, y)\n",
        "\n",
        "# Display head of processed features\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RPD3cfZkBb4"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Load data, preprocess, and train Random Forest model\n",
        "# We load the CSV, encode categorical features, standardize, and fit a RandomForestClassifier.\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('Crop Recommendation using Soil Properties and Weather Prediction (3).csv')\n",
        "# Initialize encoders\n",
        "e_color = LabelEncoder().fit(df['Soilcolor'])\n",
        "e_label = LabelEncoder().fit(df['label'])\n",
        "# Encode and prepare features\n",
        "df['Soilcolor_enc'] = e_color.transform(df['Soilcolor'])\n",
        "\n",
        "# Correct column names for X features (ensure capitalization matches the DataFrame)\n",
        "# EXCLUDE the original 'Soilcolor' column and INCLUDE the 'Soilcolor_enc' column\n",
        "X = df[['Ph','N','P','K','T2M_MAX-W','T2M_MAX-Sp','T2M_MAX-Su','T2M_MAX-Au','T2M_MIN-W','PRECTOTCORR-W','Soilcolor_enc']]\n",
        "# Separate the assignment of y to a new line\n",
        "y = e_label.transform(df['label'])\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler().fit(X) # This will now work as X contains only numeric columns\n",
        "X_scaled = scaler.transform(X)\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_scaled, y)\n",
        "\n",
        "# Display head of processed features\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmANjESh4nmN"
      },
      "outputs": [],
      "source": [
        "# Cell: Create and launch Gradio app for the trained Random Forest model\n",
        "import gradio as gr\n",
        "\n",
        "def predict_crop(pH, N, P, K, Zn, S, soilcolor):\n",
        "    # Encode soil color\n",
        "    soil_enc = e_color.transform([soilcolor])[0]\n",
        "    features = [pH, N, P, K, Zn, S, soil_enc]\n",
        "    # Scale features\n",
        "    features_scaled = scaler.transform([features])\n",
        "    # Predict and decode label\n",
        "    pred_enc = rf_model.predict(features_scaled)[0]\n",
        "    return e_label.inverse_transform([pred_enc])[0]\n",
        "\n",
        "# Define Gradio interface inputs and outputs\n",
        "demo = gr.Interface(\n",
        "    fn=predict_crop,\n",
        "    inputs=[\n",
        "        gr.Number(label='pH'),\n",
        "        gr.Number(label='N'),\n",
        "        gr.Number(label='P'),\n",
        "        gr.Number(label='K'),\n",
        "        gr.Number(label='Zn'),\n",
        "        gr.Number(label='S'),\n",
        "        gr.Dropdown(choices=e_color.classes_.tolist(), label='Soil Color')\n",
        "    ],\n",
        "    outputs=gr.Textbox(label='Recommended Crop'),\n",
        "    title='Crop Recommendation System',\n",
        "    description='Enter soil parameters to get a recommended crop (Random Forest model).'\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True)  # share=True can be added if needed\n",
        "print('Gradio app launched.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3AF2JObo4swz",
        "outputId": "b41dc81e-d167-4433-fdcf-fd8de0a72464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'e_color' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-4248202425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PRECTOTCORR-W'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PRECTOTCORR-Sp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PRECTOTCORR-Su'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PRECTOTCORR-Au'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'WD10M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GWETTOP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CLOUD_AMT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'WS2M_RANGE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Soilcolor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me_color\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'e_color' is not defined"
          ]
        }
      ],
      "source": [
        "# Cell: Define and launch a full-feature Gradio app including seasonal weather features\n",
        "import gradio as gr\n",
        "\n",
        "def predict_crop_full(Ph, N, P, K, Zn, S,\n",
        "                      QV2M_W, QV2M_Sp, QV2M_Su, QV2M_Au,\n",
        "                      T2M_MAX_W, T2M_MAX_Sp, T2M_MAX_Su, T2M_MAX_Au,\n",
        "                      T2M_MIN_W, T2M_MIN_Sp, T2M_MIN_Su, T2M_MIN_Au,\n",
        "                      PRECTOTCORR_W, PRECTOTCORR_Sp, PRECTOTCORR_Su, PRECTOTCORR_Au,\n",
        "                      WD10M, GWETTOP, CLOUD_AMT, WS2M_RANGE, PS,\n",
        "                      Soilcolor):\n",
        "    # Encode soil color\n",
        "    soil_enc = e_color.transform([Soilcolor])[0]\n",
        "    # Build feature vector in correct order\n",
        "    features = [Ph, K, P, N, Zn, S,\n",
        "                QV2M_W, QV2M_Sp, QV2M_Su, QV2M_Au,\n",
        "                T2M_MAX_W, T2M_MAX_Sp, T2M_MAX_Su, T2M_MAX_Au,\n",
        "                T2M_MIN_W, T2M_MIN_Sp, T2M_MIN_Su, T2M_MIN_Au,\n",
        "                PRECTOTCORR_W, PRECTOTCORR_Sp, PRECTOTCORR_Su, PRECTOTCORR_Au,\n",
        "                WD10M, GWETTOP, CLOUD_AMT, WS2M_RANGE, PS,\n",
        "                soil_enc]\n",
        "    # Scale and predict\n",
        "    scaled = scaler.transform([features])\n",
        "    pred = rf_model.predict(scaled)[0]\n",
        "    return e_label.inverse_transform([pred])[0]\n",
        "\n",
        "# Define inputs list\n",
        "inputs = [\n",
        "    gr.Number(label='Ph'), gr.Number(label='N'), gr.Number(label='P'), gr.Number(label='K'),\n",
        "    gr.Number(label='Zn'), gr.Number(label='S'),\n",
        "    gr.Number(label='QV2M-W'), gr.Number(label='QV2M-Sp'), gr.Number(label='QV2M-Su'), gr.Number(label='QV2M-Au'),\n",
        "    gr.Number(label='T2M_MAX-W'), gr.Number(label='T2M_MAX-Sp'), gr.Number(label='T2M_MAX-Su'), gr.Number(label='T2M_MAX-Au'),\n",
        "    gr.Number(label='T2M_MIN-W'), gr.Number(label='T2M_MIN-Sp'), gr.Number(label='T2M_MIN-Su'), gr.Number(label='T2M_MIN-Au'),\n",
        "    gr.Number(label='PRECTOTCORR-W'), gr.Number(label='PRECTOTCORR-Sp'), gr.Number(label='PRECTOTCORR-Su'), gr.Number(label='PRECTOTCORR-Au'),\n",
        "    gr.Number(label='WD10M'), gr.Number(label='GWETTOP'), gr.Number(label='CLOUD_AMT'), gr.Number(label='WS2M_RANGE'), gr.Number(label='PS'),\n",
        "    gr.Dropdown(label='Soilcolor', choices=e_color.classes_.tolist())\n",
        "]\n",
        "\n",
        "demo_full = gr.Interface(fn=predict_crop_full,\n",
        "                         inputs=inputs,\n",
        "                         outputs=gr.Textbox(label='Recommended Crop'),\n",
        "                         title='Full Crop Recommendation',\n",
        "                         description='Provide soil and seasonal weather parameters to predict the optimal crop.')\n",
        "\n",
        "demo_full.launch(share=True)\n",
        "print('Full Gradio frontend launched.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}